{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V-XG3NmQI8aF"
   },
   "outputs": [],
   "source": [
    "from robustbench.utils import clean_accuracy\n",
    "from robustbench.utils import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import unique\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiKA1t-QOzlJ"
   },
   "source": [
    "### Download and preprocess the data:\n",
    "\n",
    "* We will use 1000 test examples from the cifar 10 dataset. These images are new to the model as it hasn't seen them in the training phase. We want to fool the model on its predictions for new images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HPgPWYFJLHa",
    "outputId": "f87909f4-a137-4eda-8874-b6c02bdd8cde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1A5gQCE0bHZhBlfcLQ2fFP5UygpgVkdAX\n",
      "To: c:\\Users\\Elyas\\OneDrive - The University of Colorado Denver\\Desktop\\Projects\\decoy_challenge\\cifar10.pt\n",
      "100%|██████████| 12.3M/12.3M [00:00<00:00, 18.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cifar10.pt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "output_file = 'cifar10.pt'\n",
    "file_id = \"1A5gQCE0bHZhBlfcLQ2fFP5UygpgVkdAX\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elyas\\AppData\\Local\\Temp\\ipykernel_28040\\3968093994.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cifar_data = torch.load('cifar10.pt')\n"
     ]
    }
   ],
   "source": [
    "cifar_data = torch.load('cifar10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7D32wiSfHJHy",
    "outputId": "bdb5aa5b-8be9-4fd4-fe8f-010a75bfa15a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([100, 100, 100, 100, 100, 100, 100, 100, 100, 100]))\n"
     ]
    }
   ],
   "source": [
    "# Extract the images and labels tensors\n",
    "x_test = cifar_data['images'] / 255.0\n",
    "y_test = cifar_data['labels']\n",
    "\n",
    "print(unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Xp3JY6QJPuD",
    "outputId": "1ac8b27f-db0e-403c-f1c9-229194a93d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 32, 32]) torch.Size([1000])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape, y_test.shape)\n",
    "print(torch.max(x_test), torch.min(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiHHpdfePSr1"
   },
   "source": [
    "### Loading the robust model\n",
    "\n",
    "* IMPORTANT: You shouldn't change this part of the code as your final generated examples will be evaluated how successful you are at fooling this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZEph6jWJQKz",
    "outputId": "623d5c9b-aa7d-4e68-b70d-4dcf06cad616"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elyas\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\robustbench\\utils.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_name='Kireev2021Effectiveness_RLATAugMix', dataset='cifar10', threat_model='corruptions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUF3TMgfPzgu"
   },
   "source": [
    "### GPU Utilization\n",
    "\n",
    "* For shorter running time, let's utilize GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FTfa9zuJQH0",
    "outputId": "4bf13df2-719d-4544-d3c8-996006507574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set the device accordingly\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model = model.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czWsx4ePQIkw"
   },
   "source": [
    "### Adversarial Example Generation -- Adversarial Perturbation\n",
    "\n",
    "* Here for a baseline, we use the PGD algorithm from foolbox library. This is the most important part of the challenge. What algorithm is gonna work best?\n",
    "\n",
    "* There are many algorithms and many other adversarial example generation algorithms. Don't forget to check out other libraries!\n",
    "\n",
    "  * One other very popular library among many others is Adversarial Robustness Toolbox (ART)!\n",
    "  * There are many more algorithms out there, your task is to find the ones that works best based on our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import foolbox as fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fb = fb.PyTorchModel(model, bounds=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running attack with params: rel_stepsize=0.527, steps=300, epsilon:0.011764705882352941\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import foolbox as fb\n",
    "\n",
    "# Define the hyperparameter grid for LinfPGD\n",
    "rel_stepsize_grid = [0.527]  # Relative step sizes\n",
    "steps_grid = [300]  # Number of optimization steps\n",
    "epsilons_grid = [3/255]  # Epsilon values to keep fixed\n",
    "\n",
    "# Combine all hyperparameters into a grid\n",
    "grid = list(itertools.product(rel_stepsize_grid, steps_grid, epsilons_grid ))\n",
    "\n",
    "# Function to run the attack with the given hyperparameters\n",
    "def run_attack(model_fb, x_test, y_test, rel_stepsize, steps, epsilons):\n",
    "    return fb.attacks.LinfPGD(rel_stepsize=rel_stepsize, steps=steps)(model_fb, x_test, y_test, epsilons=epsilons)\n",
    "\n",
    "# Iterate through the hyperparameter grid\n",
    "results = []\n",
    "for params in grid:\n",
    "    rel_stepsize, steps,epsilons_grid  = params\n",
    "    print(f\"Running attack with params: rel_stepsize={rel_stepsize}, steps={steps}, epsilon:{epsilons_grid}\")\n",
    "\n",
    "    # Run the attack with current parameters\n",
    "    _, advs, success = run_attack(model_fb, x_test, y_test,\n",
    "                                  rel_stepsize=rel_stepsize,\n",
    "                                  steps=steps,\n",
    "                                  epsilons=epsilons_grid)\n",
    "\n",
    "    # Evaluate based on your custom scoring system (update this according to your project)\n",
    "    score = 1 - success.float().mean()\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'rel_stepsize': rel_stepsize,\n",
    "        'steps': steps,\n",
    "        'score': score,\n",
    "        'epsilon': epsilons_grid\n",
    "    })\n",
    "    print(f\"Score: {score}\")\n",
    "\n",
    "# After completing the grid search, find  the best set of parameters based on the score\n",
    "best_result = max(results, key=lambda x: x['score'])\n",
    "print(f\"Best result: {best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _, advs, success \u001b[38;5;241m=\u001b[39m \u001b[43mfb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinfDeepFoolAttack\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43movershoot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\foolbox\\attacks\\base.py:416\u001b[0m, in \u001b[0;36mMinimizationAttack.__call__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    413\u001b[0m     early_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(epsilons)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# run the actual attack\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m xpcs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    419\u001b[0m success \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\foolbox\\attacks\\deepfool.py:150\u001b[0m, in \u001b[0;36mDeepFoolAttack.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# then run all the other k's as well\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# we could avoid repeated forward passes and only repeat\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# the backward pass, but this cannot currently be done in eagerpy\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m diffs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mloss_aux_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# we don't need the logits\u001b[39;00m\n\u001b[0;32m    153\u001b[0m diffs_ \u001b[38;5;241m=\u001b[39m [(losses, grad) \u001b[38;5;28;01mfor\u001b[39;00m _, (losses, _), grad \u001b[38;5;129;01min\u001b[39;00m diffs]\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\foolbox\\attacks\\deepfool.py:150\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# then run all the other k's as well\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# we could avoid repeated forward passes and only repeat\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# the backward pass, but this cannot currently be done in eagerpy\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m diffs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mloss_aux_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, candidates)]\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# we don't need the logits\u001b[39;00m\n\u001b[0;32m    153\u001b[0m diffs_ \u001b[38;5;241m=\u001b[39m [(losses, grad) \u001b[38;5;28;01mfor\u001b[39;00m _, (losses, _), grad \u001b[38;5;129;01min\u001b[39;00m diffs]\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\eagerpy\\tensor\\pytorch.py:507\u001b[0m, in \u001b[0;36mPyTorchTensor._value_and_grad_fn.<locals>.value_and_grad\u001b[1;34m(x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     loss \u001b[38;5;241m=\u001b[39m f(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    506\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m--> 507\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    509\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(x\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, advs, success = fb.attacks.LinfDeepFoolAttack(steps=5, candidates=10, overshoot=0.02, loss='logits')(model_fb, x_test, y_test, epsilons=[8/255])\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxeMMZ-gRzzR"
   },
   "source": [
    "### Let's compare the accuracies before and after perturbation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OY92YmusJee2",
    "outputId": "f2d5ccfc-5ff5-4598-f738-3f4ebb2eb94a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy: 30.5%\n",
      "0.941\n"
     ]
    }
   ],
   "source": [
    "print('Robust accuracy: {:.1%}'.format(1 - success.float().mean()))\n",
    "print(clean_accuracy(model, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAGYD88bR73n"
   },
   "source": [
    "### Let's explore how our perturbations look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "q-wa0RC_OBxs",
    "outputId": "75d1319c-1c4c-473b-82fa-009580de9ef8"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Pass the perturbed images through the model to get the predicted labels\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to track gradients during inference\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     logits_adv \u001b[38;5;241m=\u001b[39m model(\u001b[43madvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Get the logits for the adversarial examples\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get the predicted labels from the logits\u001b[39;00m\n\u001b[0;32m     10\u001b[0m predicted_labels_adv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits_adv, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Pass the perturbed images through the model to get the predicted labels\n",
    "with torch.no_grad():  # No need to track gradients during inference\n",
    "    logits_adv = model(advs[0].to('cuda'))  # Get the logits for the adversarial examples\n",
    "\n",
    "# Get the predicted labels from the logits\n",
    "predicted_labels_adv = torch.argmax(logits_adv, dim=1)\n",
    "\n",
    "# Find which examples were misclassified (where predicted label != true label)\n",
    "misclassified_indices = (predicted_labels_adv != y_test.to('cuda')).nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Get the misclassified original and perturbed images, true labels, and incorrect labels\n",
    "misclassified_images = advs[0][misclassified_indices]\n",
    "misclassified_original_images = x_test.to('cuda')[misclassified_indices]\n",
    "misclassified_predicted_labels = predicted_labels_adv[misclassified_indices]\n",
    "misclassified_true_labels = y_test.to('cuda')[misclassified_indices]\n",
    "\n",
    "# Choose a random subset of misclassified images to display\n",
    "num_images_to_show = min(10, len(misclassified_images))  # Limit to 10 images for display\n",
    "random_indices = random.sample(range(len(misclassified_images)), num_images_to_show)\n",
    "\n",
    "# Class names (assuming CIFAR-10)\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Plot the original and misclassified perturbed images side by side\n",
    "plt.figure(figsize=(25, 5))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Original image\n",
    "    original_image = misclassified_original_images[idx]\n",
    "    true_label = misclassified_true_labels[idx].item()\n",
    "\n",
    "    # Perturbed image\n",
    "    perturbed_image = misclassified_images[idx]\n",
    "    incorrect_label = misclassified_predicted_labels[idx].item()\n",
    "\n",
    "    # Convert images from tensor to numpy and transpose from (C, H, W) to (H, W, C)\n",
    "    original_img = original_image.permute(1, 2, 0).cpu().numpy()\n",
    "    perturbed_img = perturbed_image.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Plot original image\n",
    "    plt.subplot(2, num_images_to_show, i+1)\n",
    "    plt.imshow(original_img, interpolation='none')\n",
    "    plt.title(f\"Original: {class_names[true_label]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot perturbed (misclassified) image\n",
    "    plt.subplot(2, num_images_to_show, num_images_to_show + i + 1)\n",
    "    plt.imshow(perturbed_img, interpolation='none')\n",
    "    plt.title(f\"Perturbed: {class_names[incorrect_label]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgEqeBozSBvG"
   },
   "source": [
    "### Finally!\n",
    "\n",
    "* Let's save our perturbed samples in a folder called 'challenge' and submit them for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "i3gPVt_AOMb4"
   },
   "outputs": [],
   "source": [
    "# Create the 'challenge' directory if it doesn't exist\n",
    "os.makedirs('challenge', exist_ok=True)\n",
    "\n",
    "# Path to save the adversarial examples\n",
    "file_path = os.path.join('challenge', 'advs.pkl')\n",
    "\n",
    "# Save the 'advs' object\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(advs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVNnjEZnSQ4b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
